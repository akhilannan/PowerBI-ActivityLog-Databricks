{"cells":[{"cell_type":"markdown","source":["# Install/Import necessary libraries"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"f93d80d6-0c2f-437c-a313-fc5b73bd7d80","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["from datetime import date, datetime, timedelta\nfrom pyspark.sql.types import StringType, DateType\nimport requests\nimport json\nfrom pathlib import Path"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"3861e5b4-75a6-43d5-adfc-c0efbfd32959","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["# Create Function to Get Access Token and Expiry Time"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"be6737b9-d6bd-48fc-819a-9f9a25485401","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["def GetAccessToken(client_id, client_secret, authority_url, scope):\n    # Get Token\n    data = { 'grant_type':'client_credentials',\n     'client_id': client_id,\n     'client_secret': client_secret,\n     'resource': scope }\n    result = requests.post(url=authority_url, data=data).json()\n    #Extract token info\n    access_token = result['token_type'] + \" \" + result['access_token']\n    expires_in = int(result['expires_in']) - 300\n    expires_at = datetime.now() + timedelta(seconds=expires_in)\n    return access_token, expires_at"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"5b56d4ac-d628-44b0-880c-509c3c3ac4eb","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["# UDF for Calling REST API"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"5fad536d-ace6-422e-9f26-ea1a1d9655e1","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["@udf(returnType=StringType())\ndef do_requests(rawpath, fileName, access_token, RunDate):\n    # Initialize variables\n    activityDate = RunDate.strftime(\"%Y-%m-%d\")\n    activityYear = RunDate.strftime(\"%Y\")\n    activityMonth = RunDate.strftime(\"%m\")\n    activityDay = RunDate.strftime(\"%d\")\n    url = \"https://api.powerbi.com/v1.0/myorg/admin/activityevents?startDateTime='\" + activityDate + \"T00:00:00.000'&endDateTime='\" + activityDate + \"T23:59:59.999'\"\n    incrementPath = f\"{rawpath}/{activityYear}/{activityMonth}/{activityDay}\"\n    writePath = incrementPath[1:]\n    \n    # Get latest Power BI Activities & Set continuation URL\n    header = {'Content-Type':'application/json', 'Authorization':f'{access_token}'}\n    api_call = requests.get(url=url, headers=header).json()\n    activityEventEntities = api_call.get('activityEventEntities', [])\n    contUrl = api_call.get('continuationUri')\n\n    # Call Continuation URL as long as results get one back to get all activities through the day\n    while contUrl is not None:   \n        api_call_cont = requests.get(url=contUrl, headers=header).json()\n        contUrl = api_call_cont['continuationUri']\n        activityEventEntities = activityEventEntities + api_call_cont['activityEventEntities']\n\n    if activityEventEntities:\n        # Serializing json \n        json_object = json.dumps(activityEventEntities, indent = 4)\n        \n        #Create folder\n        Path(writePath).mkdir(parents=True, exist_ok=True)\n        \n        # Wrting to json file\n        with open(f'{writePath}/{fileName}', \"w\") as outfile:\n            outfile.write(json_object)\n        \n        return incrementPath"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"16c482cc-2ea7-4d63-be36-edbd62ba963d","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["# Calling UDF in Parallel via Spark Dataframe"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"3b05a39d-0bfb-4c0d-88a6-d907498b9c26","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["def parallel_request(dateList, basePath, rawpath, fileName, access_token):\n    from pyspark.sql.functions import lit, col\n    path = [data[0] for data in spark\n                     .createDataFrame(dateList, DateType())\n                     .select(do_requests(lit(rawpath), lit(fileName), lit(access_token), col(\"value\")).alias(\"result\"))\n                     .filter(col(\"result\").isNotNull())\n                     .collect()]\n    dbutils.fs.cp(f\"file:///databricks/driver{basePath}\", f\"dbfs:{basePath}\", True)\n    dbutils.fs.rm(f\"file:///databricks/driver{basePath}\", True)\n    return path"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"51ee4475-af8b-4ca4-8ad7-7dd3d47053a3","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["# Create Function to generate date list between start and end date"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"c4c20254-98ba-4d7e-ac17-4e3e0f8f0841","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["def generate_date_list(startDate, endDate):\n    return [startDate+timedelta(days=x) for x in range((endDate-startDate).days+1)]"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"3dc264ff-a27d-4028-af04-233f322d3239","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["# Create Function to check if Delta table exists"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"51957d78-f30c-4c5c-87db-1e429c171e68","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["def delta_table_exists(path):\n    from delta.tables import DeltaTable\n    try:\n        DeltaTable.forPath(spark, path)\n        return True\n    except Exception as e:\n        return False"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"11d91e97-7d35-4729-92ea-4e8aa7e79621","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["# Function to read json Data"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"4be7920f-4b3a-4f02-bb1d-4c2b4eba9f95","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["def dfRead(fileType, path, schema=None):\n    if fileType.lower() == 'delta':\n        return (spark\n          .read\n          .format(fileType)\n          .option(\"multiline\", \"true\")\n          .load(path))\n    else:\n        return (spark\n          .read\n          .format(fileType)\n          .option(\"inferSchema\", \"false\")\n          .schema(schema)\n          .option(\"recursiveFileLookup\",\"true\")\n          .option(\"multiline\", \"true\")\n          .load(path))"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"639e8b24-153e-4c19-af05-b8c30c7dd456","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["# Function to Optimize and Vacuum table"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"063015a5-2b2b-43fb-b524-b7a75e73a404","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["def OptimizeAndVacuum(tableName, ZOrder=None, RetainHours=0):\n    spark.conf.set(\"spark.databricks.delta.retentionDurationCheck.enabled\",False)\n    optimizeBase = f\"OPTIMIZE {tableName}\"\n    ZOrderBase = f\" ZORDER BY ({ZOrder})\" if ZOrder is not None else \"\"\n    optimizeFull = optimizeBase + ZOrderBase\n    VacuumFull = f\"VACUUM {tableName} RETAIN {RetainHours} HOURS\"\n    spark.sql(optimizeFull)\n    spark.sql(VacuumFull)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"250994f3-296e-456b-9fff-d94fe7526545","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"Common Functions","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{},"notebookOrigID":161498550596580}},"nbformat":4,"nbformat_minor":0}
